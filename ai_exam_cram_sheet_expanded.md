# 速记宝典（扩展版）- 数据处理 / 基础理论 / 深度学习 / 计算机视觉

适合非专业考前快速理解与记忆；每节含：是什么/为什么/怎么做/易错点/必背清单/例题模版。

---

## 2. 数据处理（Data Processing）

是什么：把“原始数据”变成“可建模数据”的流水线。

- 一图记流程：
  采集 → 理解 → 清洗 → 特征工程 → 切分验证 → 处理不均衡 → 训练评估 → 上线监控

- 数据类型与“度量尺度”（常考）：
  - 名义（定类）：只表示类别，如性别/城市（能=、≠，不能比较大小）
  - 有序（定序）：有顺序，无固定间隔，如评分“差/中/优”
  - 区间：有顺序、等间隔、无绝对零点，如温度℃（20和10差10，但20不是10的两倍）
  - 比率（定比）：有绝对零点、比例有意义，如工资/身高/重量（工资=定比等级数据）

- 缺失值处理（MCAR/MAR/MNAR）：
  - MCAR：完全随机缺失，删除样本影响小；
  - MAR：与可观测变量相关，如高龄人群更易缺；
  - MNAR：与缺失值本身相关，如收入高的人不愿报收入。
  - 策略：
    - 删除：样本少/MCAR；
    - 简单填充：数值（均值/中位数），分类（众数）；
    - 高级填充：KNN、回归、多重插补（MICE）；
    - 保留“缺失指示”特征；
    - 切记：先在训练集“拟合填充值”，再应用到验证/测试，避免数据泄露。

- 异常值处理（Outlier）：
  - 检测：Z-score>|3|；IQR法（>Q3+1.5IQR 或 <Q1-1.5IQR）；箱线图；
  - 处理：截断/替换（winsorize）、对数变换、使用鲁棒模型（如MAE/Huber、树模型）。

- 特征工程：
  - 编码：
    - 独热（One-Hot）：安全但维度可能爆炸；
    - 标签编码（Ordinal）：仅当类别有序（否则引入伪顺序）；
    - 目标/均值编码：高效但需交叉验证防泄露；
    - 频次/哈希编码：高基数类别常用。
  - 缩放：
    - 归一化[0,1]：min-max，受极值影响；
    - 标准化：z-score，更稳健；KNN/聚类/距离模型前必做。
  - 分箱：等宽/等频/卡方分箱，提升鲁棒、适配树模型；
  - 选择：
    - Filter（方差/相关/卡方/互信息）、Wrapper（递归特征消除RFE）、Embedded（L1稀疏、树模型重要度）。

- 划分与验证：
  - 数据集：训练/验证/测试；比例常见 6/2/2 或 8/1/1；
  - 交叉验证：K折、留一；
  - 时间序列：滚动/扩展窗口，严禁打乱时间；
  - 防泄露顺序：先切分→再拟合变换→最后整体应用。

- 类不平衡（Positive极少）：
  - 指标优先：AUC/PR（PR更敏感），不要迷信准确率；
  - 方法：类权重/阈值移动/欠采样/过采样（SMOTE、ADASYN）/集成（BalancedBagging、EasyEnsemble）。

- 评估指标（分类）：
  - 混淆矩阵：TP/FP/FN/TN；
  - 精准率P=TP/(TP+FP)，召回率R=TP/(TP+FN)，F1=2PR/(P+R)；
  - ROC曲线（TPR vs FPR）与AUC；PR曲线（P vs R）更适合极不均衡。

- 易错点（考试爱考）：
  - 距离度量模型（KNN/聚类/距离SVM）必须先缩放；
  - One-Hot高维可换目标/频率编码；
  - 时间序列不能随机划分；
  - 填充值不得用全量数据统计；
  - 评估指标要匹配任务，不均衡优先AUC/PR。

- 必背清单：
  - 工资=定比；数据格式不包括“光纤”；不均衡评估=AUC/PR；
  - 图像预处理常见：resize/归一化/数据增强；文本：分词/停用词/TF-IDF。

---

## 3. 基础理论（Basic Theory）

是什么：机器学习“底层三要素”与统计/信息论基础。

- 三要素：
  - 模型（假设空间）：线回归、树、SVM、神经网络…
  - 策略（损失/评估）：最小化经验风险（训练误差）、结构风险=经验风险+正则；
  - 算法（求解）：梯度下降、坐标下降、动态规划等。

- 学习范式（常考“4种”）：
  - 监督：有标签（分类/回归）；
  - 无监督：无标签（聚类/降维/密度）；
  - 半监督：少量标签+大量无标签；
  - 强化学习：与环境交互、累积回报（棋对弈/自动驾驶/路径规划）。

- 偏差-方差权衡：
  - 欠拟合：偏差高、方差低；过拟合：偏差低、方差高；
  - 解法：更多数据、正则化（L1稀疏/L2平滑）、数据增强、早停、交叉验证、模型简化。

- 概率与贝叶斯：
  - 贝叶斯公式：P(A|B)=P(B|A)P(A)/P(B)；
  - MLE：只最大化似然；MAP：最大化后验=似然×先验（相当于加入正则/先验偏好）。

- 信息论与常用损失：
  - 熵H：不确定性；交叉熵：衡量两分布差异（分类常用）；KL散度：P相对Q的信息损失；
  - 分类：交叉熵、Hinge；回归：MSE（对离群敏感）/MAE（更鲁棒）/Huber（折中）。

- 正则化：
  - L1：稀疏、做特征选择；L2：权重更平滑，抗共线性；
  - Dropout/数据增强/早停也起到“正则”效果。

- 优化基础：
  - 梯度下降（批/小批/随机）；动量/牛顿/拟牛顿；学习率与收敛；
  - 凸优化易全局最优；深度学习多为非凸但实践可行；
  - 约束优化用拉格朗日乘子/KKT（理解概念即可）。

- 易错点：
  - “交叉熵越小越好”要配合验证集与正则；
  - L1更稀疏，L2更稳；
  - 精准率/召回率由阈值影响，不是模型天生固定。

- 必背清单：
  - 学习范式=监督/无监督/半监督/强化；
  - SRM=ERM+正则；ROC横轴FPR、纵轴TPR；
  - 回归损失：MSE/MAE/Huber；分类：交叉熵/Hinge。

---

## 4. 深度学习（Deep Learning）

是什么：多层非线性网络，用反向传播+梯度优化从大数据中学习表示。

- 基本流程：输入 → 线性变换（卷积/全连接）+激活 → 堆叠多层 → 损失 → 反向传播（链式求导） → 优化器更新。

- 激活与初始化：
  - ReLU族（ReLU/LeakyReLU/GELU）主流；Sigmoid/Tanh易饱和致梯度消失；
  - Xavier/He初始化与激活匹配，稳定方差传播。

- 优化与学习率：
  - 优化器：SGD+Momentum、RMSProp、Adam（收敛快，注意权重衰减的“解耦”AdamW）；
  - 学习率调度：Step/余弦退火/热身（Warmup）；
  - 技巧：梯度裁剪（防爆炸）、Batch大小与泛化权衡、混合精度（提速）。

- 归一化与正则：
  - BatchNorm（批内均值方差，CNN常用）、LayerNorm（序列/Transformer常用）；
  - Dropout/数据增强/权重衰减（L2/AdamW）/早停。

- CNN 精要：
  - 卷积核/步幅/填充决定特征图尺寸与感受野；
  - 池化（最大/平均）降采样、抑制噪声；
  - 1×1卷积做通道混合与降维；
  - 残差连接（ResNet）解退化、利于训练深网；
  - 深度可分离卷积（MobileNet）提速降参。

- 序列模型：
  - RNN：长序列梯度消失/爆炸；
  - LSTM/GRU：门控机制缓解长期依赖；
  - 常见任务：文本/语音/时序预测。

- Transformer 要点：
  - 自注意力（Q/K/V、缩放点积、多头）；
  - 位置编码（绝对/相对）；
  - 编码器/解码器结构；
  - 视觉迁移：ViT（图像块化+位置编码）、DETR（检测=注意力+匈牙利匹配）。

- 任务与损失：
  - 分类：交叉熵（可配Label Smoothing）；
  - 检测：分类损失+回归损失（L1/IoU/GIoU/DIoU/CIoU）；
  - 语义分割：像素级交叉熵/Dice；
  - 序列对齐：CTC。

- 易错点与必背：
  - “训练优化算法”常选：梯度下降及其变体；
  - “序列数据易梯度消失的模型”：RNN（非LSTM/GRU/CNN）；
  - “深度学习数据来源”常选：互联网；
  - 过拟合优先：数据增强/正则/早停/合适学习率，而非盲目加层。

---

## 6. 计算机视觉（Computer Vision）

是什么：让计算机“看懂”图像/视频的技术体系。

- 图像与颜色基础：
  - 像素/通道；常见色彩空间：RGB、BGR（OpenCV）、HSV/YCbCr；
  - 直方图均衡化：提升对比度；Gamma校正；
  - 归一化：像素缩放到[0,1]或按均值方差标准化（如ImageNet均值方差）。

- 平滑与锐化：
  - 平滑（去噪）：均值/高斯/中值（椒盐噪声克星）；
  - 锐化（增强边缘/高频）：高通滤波、Sobel/Prewitt、Laplacian、LoG；
  - 考点：图像“锐化处理”属于高通滤波；中值滤波是平滑，不是边缘检测。

- 边缘检测：
  - Canny流程：高斯去噪→梯度计算→非极大值抑制→双阈值连接；
  - 阈值选择影响断裂与噪声；
  - 形态学：腐蚀/膨胀/开（去小噪）/闭（填小洞），基于结构元素。

- 特征与匹配：
  - 角点：Harris、Shi-Tomasi；
  - 局部特征：SIFT/SURF（尺度旋转不变）、ORB（快速二值）；
  - 描述子匹配+RANSAC做鲁棒几何估计（剔除离群）。

- 典型任务与指标：
  - 分类：Top-1/Top-k准确率；
  - 检测：候选框、IoU、NMS抑制重叠、mAP；
  - 分割：mIoU/Dice；
  - 重建/压缩：PSNR/SSIM。

- 深度视觉模型：
  - 分类：ResNet、DenseNet、ViT；
  - 检测：Faster R-CNN、SSD、YOLO、DETR；
  - 分割：U-Net、DeepLab。

- 数据与增强：
  - 数据集：VOC/COCO（检测与分割标注规范）；
  - 增强：翻转/旋转/缩放/裁剪/颜色抖动/归一化；
  - 布局：NCHW（PyTorch）/NHWC（TensorFlow）。

- 易错点与必背：
  - 边缘检测“不正确”的选项多为“中值滤波”；
  - 实时多媒体端到端实时传输协议：RTP（非TCP/UDP/RTCP）。

---

## 速记与做题策略

- 指标配任务：分类→准确/F1/ROC-AUC；检测→mAP；分割→mIoU；重建→PSNR/SSIM。
- 先后顺序：先切分再标准化；先增强再训练；先特征后建模。
- 排除法：出现“总是/完全/唯一/必须”类绝对词的选项要警惕（定义题除外）。

## 迷你练习（带答案）

1) 下列哪类问题不适合用强化学习？A. 棋对弈 B. 自动驾驶 C. 交通灯识别 D. 路径规划  答：C（监督视觉分类更合适）。
2) 图像锐化处理更接近：A. 局部平均 B. 最均匀平滑 C. 高通滤波 D. 中值滤波  答：C。
3) 边缘检测算法中不正确的是：A. 梯度算子 B. 方向算子 C. 中值滤波 D. 拉普拉斯算子  答：C。
4) 定比等级数据的例子：A. 工作种类 B. 工资 C. 血型 D. 成绩等级  答：B。
5) 深度学习训练常用优化算法：A. 线性回归 B. K-means C. 梯度下降变体 D. PageRank  答：C。

---

（完）
